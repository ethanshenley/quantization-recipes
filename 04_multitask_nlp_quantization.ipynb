{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning and Quantization for Multi-task NLP\n",
    "\n",
    "This notebook demonstrates how to perform transfer learning and quantization for multi-task NLP using a large pre-trained language model.\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1. Setup and Imports\n",
    "2. Load Pre-trained Model and Datasets\n",
    "3. mplement Task-specific Heads\n",
    "4. Mixed Precision Multi-task Fine-tuning\n",
    "5. Quantization-Aware Fine-tuning (QAF) for Shared Base\n",
    "6. Gradual Quantization of Task-specific Heads\n",
    "7. Layer-wise Adaptive Quantization\n",
    "8. Efficient INT8 Inference Implementation\n",
    "9. Critical Layer Analysis and Precision Adjustment\n",
    "10. Quantized Knowledge Distillation\n",
    "11. Performance Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from transformers import RobertaModel, RobertaTokenizerFast, AdamW, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Pre-trained Model and Datasets\n",
    "\n",
    "Let's load a pre-trained RoBERTa model and multiple NLP datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'roberta-base'\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_name, add_prefix_space=True)\n",
    "model = RobertaModel.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Load datasets for multiple tasks\n",
    "sentiment_dataset = load_dataset(\"glue\", \"sst2\")\n",
    "ner_dataset = load_dataset(\"conll2003\")\n",
    "text_classification_dataset = load_dataset(\"ag_news\")\n",
    "\n",
    "# Preprocessing function for tokenization\n",
    "def tokenize_function(examples, task):\n",
    "    if task == 'sentiment':\n",
    "        return tokenizer(examples['sentence'], padding='max_length', truncation=True, max_length=128)\n",
    "    elif task == 'classification':\n",
    "        return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)\n",
    "    elif task == 'ner':\n",
    "        tokenized_inputs = tokenizer(examples['tokens'], padding='max_length', truncation=True, is_split_into_words=True)\n",
    "        \n",
    "        # Align the labels (NER tags) with tokenized words\n",
    "        labels = []\n",
    "        for i, label in enumerate(examples['ner_tags']):\n",
    "            word_ids = tokenized_inputs.word_ids(batch_index=i)  # Get word_ids for alignment\n",
    "            previous_word_idx = None\n",
    "            label_ids = []\n",
    "            for word_idx in word_ids:\n",
    "                if word_idx is None:  # Padding token, ignore\n",
    "                    label_ids.append(-100)\n",
    "                elif word_idx != previous_word_idx:  # First token of a word\n",
    "                    label_ids.append(label[word_idx])\n",
    "                else:  # Other tokens in a word\n",
    "                    label_ids.append(-100)\n",
    "                previous_word_idx = word_idx\n",
    "            \n",
    "            # Ensure that the length of the labels is the same as the tokenized input\n",
    "            label_ids += [-100] * (128 - len(label_ids))  # Pad the labels\n",
    "            labels.append(label_ids)\n",
    "        \n",
    "        tokenized_inputs[\"labels\"] = labels\n",
    "        return tokenized_inputs\n",
    "\n",
    "# Apply tokenization\n",
    "sentiment_dataset = sentiment_dataset.map(lambda x: tokenize_function(x, task='sentiment'), batched=True)\n",
    "ner_dataset = ner_dataset.map(lambda x: tokenize_function(x, task='ner'), batched=True)\n",
    "text_classification_dataset = text_classification_dataset.map(lambda x: tokenize_function(x, task='classification'), batched=True)\n",
    "\n",
    "# Remove columns that are no longer needed for training after tokenization\n",
    "sentiment_dataset = sentiment_dataset.remove_columns(['sentence', 'idx'])\n",
    "ner_dataset = ner_dataset.remove_columns(['tokens', 'pos_tags', 'chunk_tags', 'ner_tags'])\n",
    "text_classification_dataset = text_classification_dataset.remove_columns(['text', 'label'])\n",
    "\n",
    "# Data Collator for padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer, return_tensors=\"pt\")\n",
    "\n",
    "# Prepare dataloaders\n",
    "dataloaders = {\n",
    "    'sentiment': DataLoader(sentiment_dataset['train'], batch_size=32, shuffle=True, collate_fn=data_collator),\n",
    "    'ner': DataLoader(ner_dataset['train'], batch_size=32, shuffle=True, collate_fn=data_collator),\n",
    "    'classification': DataLoader(text_classification_dataset['train'], batch_size=32, shuffle=True, collate_fn=data_collator)\n",
    "}\n",
    "\n",
    "print(\"Tokenization and DataLoader setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement Task-specific Heads\n",
    "\n",
    "Now, let's implement task-specific heads for each NLP task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multi-task model\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.sentiment_head = nn.Linear(base_model.config.hidden_size, 2)\n",
    "        self.ner_head = nn.Linear(base_model.config.hidden_size, 9)  \n",
    "        self.classification_head = nn.Linear(base_model.config.hidden_size, 4)  \n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, task):\n",
    "        base_output = self.base_model(input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        if task == 'sentiment':\n",
    "            return self.sentiment_head(base_output[:, 0, :])  \n",
    "        elif task == 'ner':\n",
    "            return self.ner_head(base_output)\n",
    "        elif task == 'classification':\n",
    "            return self.classification_head(base_output[:, 0, :])  \n",
    "\n",
    "multi_task_model = MultiTaskModel(model)\n",
    "multi_task_model = multi_task_model.to(device)\n",
    "print(\"Implemented task-specific heads.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Mixed Precision Multi-task Fine-tuning\n",
    "\n",
    "Let's implement mixed precision training for multi-task fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, dataloaders, optimizer, scheduler, scaler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for task, dataloader in dataloaders.items():\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            # Access the correct label key for each task\n",
    "            if task == 'sentiment' or task == 'classification':\n",
    "                labels = batch['labels'].to(device)\n",
    "            elif task == 'ner':\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "            with autocast():\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, task=task)\n",
    "                loss = nn.CrossEntropyLoss()(outputs.view(-1, outputs.shape[-1]), labels.view(-1))\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / sum(len(dl) for dl in dataloaders.values())\n",
    "\n",
    "# Fine-tuning setup\n",
    "optimizer = AdamW(multi_task_model.parameters(), lr=5e-5)\n",
    "num_epochs = 3\n",
    "total_steps = num_epochs * sum(len(dl) for dl in dataloaders.values())\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=5e-5, total_steps=total_steps)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Fine-tuning loop\n",
    "for epoch in range(num_epochs):\n",
    "    avg_loss = train_epoch(multi_task_model, dataloaders, optimizer, scheduler, scaler)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Mixed precision multi-task fine-tuning completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quantization-Aware Fine-tuning (QAF) for Shared Base\n",
    "\n",
    "Now, let's apply Quantization-Aware Fine-tuning to the shared base:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_qaf_to_base(model):\n",
    "    model.base_model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "    model.base_model = torch.quantization.prepare_qat(model.base_model)\n",
    "    return model\n",
    "\n",
    "multi_task_model = apply_qaf_to_base(multi_task_model)\n",
    "print(\"Applied Quantization-Aware Fine-tuning to shared base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Gradual Quantization of Task-specific Heads\n",
    "\n",
    "Let's implement gradual quantization of task-specific heads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradual_head_quantization(model, epoch, total_epochs):\n",
    "    if epoch >= total_epochs // 2:  # Start quantizing heads halfway through\n",
    "        heads = [model.sentiment_head, model.ner_head, model.classification_head]\n",
    "        for head in heads:\n",
    "            head.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "            torch.quantization.prepare_qat(head, inplace=True)\n",
    "\n",
    "# Modify the training loop to include gradual head quantization\n",
    "for epoch in range(num_epochs):\n",
    "    gradual_head_quantization(multi_task_model, epoch, num_epochs)\n",
    "    avg_loss = train_epoch(multi_task_model, dataloaders, optimizer, scheduler, scaler)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Completed gradual quantization of task-specific heads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Layer-wise Adaptive Quantization\n",
    "\n",
    "Implement layer-wise adaptive quantization based on sensitivity analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask, task='classification')  # Assume classification task for simplicity\n",
    "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = total_correct / total_samples\n",
    "    return avg_loss\n",
    "\n",
    "def sensitivity_analysis(model, dataloader):\n",
    "    sensitivities = {}\n",
    "    for name, module in model.base_model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            original_weight = module.weight.data.clone()\n",
    "            module.weight.data = torch.quantize_per_tensor(original_weight, 1.0, 0, torch.qint8).dequantize()\n",
    "            loss = evaluate_model(model, dataloader)\n",
    "            sensitivities[name] = loss\n",
    "            module.weight.data = original_weight\n",
    "    return sensitivities\n",
    "\n",
    "def apply_adaptive_quantization(model, sensitivities, threshold):\n",
    "    for name, module in model.base_model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if sensitivities[name] > threshold:\n",
    "                module.qconfig = torch.quantization.float_qparams_weight_only_qconfig\n",
    "            else:\n",
    "                module.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "\n",
    "# Perform sensitivity analysis\n",
    "sensitivities = sensitivity_analysis(multi_task_model, next(iter(dataloaders.values())))\n",
    "threshold = np.mean(list(sensitivities.values())) + np.std(list(sensitivities.values()))\n",
    "apply_adaptive_quantization(multi_task_model, sensitivities, threshold)\n",
    "\n",
    "print(\"Applied layer-wise adaptive quantization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Efficient INT8 Inference Implementation\n",
    "\n",
    "Implement efficient INT8 inference for the base model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_int8(model):\n",
    "    model.eval()\n",
    "    model.base_model = torch.quantization.convert(model.base_model)\n",
    "    return model\n",
    "\n",
    "multi_task_model = convert_to_int8(multi_task_model)\n",
    "print(\"Implemented efficient INT8 inference for base model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Critical Layer Analysis and Precision Adjustment\n",
    "\n",
    "Identify and adjust precision for critical layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_critical_layers(model, sensitivities, top_k=3):\n",
    "    sorted_sensitivities = sorted(sensitivities.items(), key=lambda x: x[1], reverse=True)\n",
    "    critical_layers = [name for name, _ in sorted_sensitivities[:top_k]]\n",
    "    \n",
    "    for name, module in model.base_model.named_modules():\n",
    "        if name in critical_layers:\n",
    "            module.qconfig = None  # Keep in higher precision\n",
    "\n",
    "adjust_critical_layers(multi_task_model, sensitivities)\n",
    "print(\"Adjusted precision for critical layers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Quantized Knowledge Distillation\n",
    "\n",
    "Implement quantized knowledge distillation to create a smaller, efficient multi-task model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallMultiTaskModel(nn.Module):\n",
    "    def __init__(self, hidden_size=256):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=hidden_size, nhead=4), num_layers=4)\n",
    "        self.sentiment_head = nn.Linear(hidden_size, 2)\n",
    "        self.ner_head = nn.Linear(hidden_size, 9)\n",
    "        self.classification_head = nn.Linear(hidden_size, 4)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, task):\n",
    "        # Simplified encoding (replace with appropriate embedding)\n",
    "        x = torch.nn.functional.one_hot(input_ids, num_classes=30522).float()\n",
    "        x = self.encoder(x)\n",
    "        if task == 'sentiment':\n",
    "            return self.sentiment_head(x[:, 0, :])\n",
    "        elif task == 'ner':\n",
    "            return self.ner_head(x)\n",
    "        elif task == 'classification':\n",
    "            return self.classification_head(x[:, 0, :])\n",
    "\n",
    "small_model = SmallMultiTaskModel().to(device)\n",
    "\n",
    "def distillation_loss(student_outputs, teacher_outputs, labels, temperature=2.0):\n",
    "    soft_targets = nn.functional.softmax(teacher_outputs / temperature, dim=1)\n",
    "    soft_prob = nn.functional.log_softmax(student_outputs / temperature, dim=1)\n",
    "    distillation_loss = nn.KLDivLoss(reduction='batchmean')(soft_prob, soft_targets) * (temperature ** 2)\n",
    "    student_loss = nn.CrossEntropyLoss()(student_outputs, labels)\n",
    "    return 0.5 * (distillation_loss + student_loss)\n",
    "\n",
    "def train_distillation(teacher_model, student_model, dataloaders, optimizer, scheduler, scaler, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for task, dataloader in dataloaders.items():\n",
    "            for batch in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                with autocast():\n",
    "                    with torch.no_grad():\n",
    "                        teacher_outputs = teacher_model(input_ids, attention_mask=attention_mask, task=task)\n",
    "                    student_outputs = student_model(input_ids, attention_mask=attention_mask, task=task)\n",
    "                    loss = distillation_loss(student_outputs, teacher_outputs, labels)\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Distillation Epoch {epoch+1}/{epochs}, Average Loss: {total_loss / sum(len(dl) for dl in dataloaders.values()):.4f}\")\n",
    "\n",
    "optimizer = AdamW(small_model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-4, total_steps=5 * sum(len(dl) for dl in dataloaders.values()))\n",
    "scaler = GradScaler()\n",
    "\n",
    "train_distillation(multi_task_model, small_model, dataloaders, optimizer, scheduler, scaler)\n",
    "print(\"Completed quantized knowledge distillation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Performance Evaluation\n",
    "\n",
    "Finally, let's evaluate the performance of our original, quantized, and distilled models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloaders):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for task, dataloader in dataloaders.items():\n",
    "            for batch in dataloader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids, attention_mask=attention_mask, task=task)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_correct += (predicted == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "    \n",
    "    return total_correct / total_samples\n",
    "\n",
    "# Prepare evaluation dataloaders\n",
    "eval_dataloaders = {\n",
    "    'sentiment': DataLoader(sentiment_dataset['validation'], batch_size=32),\n",
    "    'ner': DataLoader(ner_dataset['validation'], batch_size=32),\n",
    "    'classification': DataLoader(text_classification_dataset['test'], batch_size=32)\n",
    "}\n",
    "\n",
    "original_accuracy = evaluate_model(multi_task_model, eval_dataloaders)\n",
    "quantized_accuracy = evaluate_model(convert_to_int8(multi_task_model), eval_dataloaders)\n",
    "distilled_accuracy = evaluate_model(small_model, eval_dataloaders)\n",
    "\n",
    "print(f\"Original Model Accuracy: {original_accuracy:.4f}\")\n",
    "print(f\"Quantized Model Accuracy: {quantized_accuracy:.4f}\")\n",
    "print(f\"Distilled Model Accuracy: {distilled_accuracy:.4f}\")\n",
    "\n",
    "# Measure inference time\n",
    "def benchmark_inference_time(model, input_shape, num_runs=100):\n",
    "    model.eval()\n",
    "    input_ids = torch.randint(0, 30522, input_shape).to(device)\n",
    "    attention_mask = torch.ones(input_shape).to(device)\n",
    "    \n",
    "    start_time = torch.cuda.Event(enable_timing=True)\n",
    "    end_time = torch.cuda.Event(enable_timing=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Warm-up run\n",
    "        for _ in range(10):\n",
    "            _ = model(input_ids, attention_mask=attention_mask, task='sentiment')\n",
    "        \n",
    "        # Timed runs\n",
    "        start_time.record()\n",
    "        for _ in range(num_runs):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
