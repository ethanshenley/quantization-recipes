{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing a Large Language Model for Low-Latency Inference\n",
    "\n",
    "This notebook demonstrates how to optimize a pre-trained large language model for low-latency inference using quantization and other optimization techniques.\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1. Setup and Imports\n",
    "2. Load Pre-trained Model\n",
    "3. Mixed Precision Fine-tuning\n",
    "4. Post-Training Quantization (PTQ)\n",
    "5. Efficient INT8 GEMM Operations\n",
    "6. Quantized Key-Value Caching\n",
    "7. Dynamic Quantization for Activations\n",
    "8. Deployment with ONNX Runtime\n",
    "9. Performance Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, lets import the necessary liberaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Pre-trained Model\n",
    "\n",
    "We'll use a pre-trained GPT-2 model for this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Add a padding token (use eos_token or a new token)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "    model.resize_token_embeddings(len(tokenizer))  # Adjust token embeddings for the new token\n",
    "        \n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mixed Precision Fine-tuning\n",
    "\n",
    "Now, let's implement mixed precision fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune(model, train_dataloader, optimizer, epochs=3):\n",
    "    scaler = torch.amp.GradScaler()\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.cuda.amp.autocast():  # autocast for mixed precision\n",
    "                inputs = batch[\"input_ids\"].to(device)\n",
    "                labels = inputs.clone()\n",
    "                outputs = model(inputs, labels=labels)\n",
    "                loss = outputs.loss\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "\n",
    "# Tokenize the dataset properly\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Convert the dataset to the format expected by the model\n",
    "tokenized_dataset = tokenized_dataset.with_format(\"torch\")\n",
    "\n",
    "# Use a data collator to ensure uniform batches and tensor conversion\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataloader = DataLoader(tokenized_dataset, batch_size=4, shuffle=True, collate_fn=data_collator)\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Fine-tune the model\n",
    "fine_tune(model, train_dataloader, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Post-Training Quantization (PTQ)\n",
    "\n",
    "Now that we have fine-tuned our model, let's apply Post-Training Quantization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.quantization\n",
    "\n",
    "def apply_ptq(model):\n",
    "    # Specify quantization configuration\n",
    "    model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "    \n",
    "    # Prepare model for quantization\n",
    "    model_prepared = torch.quantization.prepare(model)\n",
    "    \n",
    "    # Calibrate the model (you would typically do this with a calibration dataset)\n",
    "    with torch.no_grad():\n",
    "        for batch in train_dataloader:\n",
    "            inputs = batch[\"input_ids\"].to(device)\n",
    "            model_prepared(inputs)\n",
    "    \n",
    "    # Convert to quantized model\n",
    "    model_quantized = torch.quantization.convert(model_prepared)\n",
    "    \n",
    "    # Keep embedding layers and softmax in FP16\n",
    "    model_quantized.transformer.wte = model.transformer.wte.half()\n",
    "    model_quantized.transformer.wpe = model.transformer.wpe.half()\n",
    "    model_quantized.lm_head = model.lm_head.half()\n",
    "    \n",
    "    return model_quantized\n",
    "\n",
    "model_int8 = apply_ptq(model.cpu())  # PTQ requires CPU\n",
    "print(\"Model quantized to INT8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implement efficient INT8 GEMM operations for attention mechanisms\n",
    "\n",
    "For this step, we'll use PyTorch's built-in quantized linear layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedAttention(nn.Module):\n",
    "    def __init__(self, attention):\n",
    "        super().__init__()\n",
    "        self.query = torch.quantization.quantize_dynamic(\n",
    "            attention.query, {torch.nn.Linear}, dtype=torch.qint8\n",
    "        )\n",
    "        self.key = torch.quantization.quantize_dynamic(\n",
    "            attention.key, {torch.nn.Linear}, dtype=torch.qint8\n",
    "        )\n",
    "        self.value = torch.quantization.quantize_dynamic(\n",
    "            attention.value, {torch.nn.Linear}, dtype=torch.qint8\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        query_layer = self.query(hidden_states)\n",
    "        key_layer = self.key(hidden_states)\n",
    "        value_layer = self.value(hidden_states)\n",
    "        return query_layer, key_layer, value_layer\n",
    "\n",
    "# Replace attention layers with quantized versions\n",
    "for layer in model_int8.transformer.h:\n",
    "    layer.attn = QuantizedAttention(layer.attn)\n",
    "\n",
    "print(\"Implemented efficient INT8 GEMM operations for attention mechanisms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Apply quantized key-value caching for faster autoregressive inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CachedQuantizedAttention(QuantizedAttention):\n",
    "    def __init__(self, attention):\n",
    "        super().__init__(attention)\n",
    "        self.cached_key = None\n",
    "        self.cached_value = None\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        query_layer = self.query(hidden_states)\n",
    "        \n",
    "        if self.cached_key is None:\n",
    "            key_layer = self.key(hidden_states)\n",
    "            value_layer = self.value(hidden_states)\n",
    "            self.cached_key = key_layer\n",
    "            self.cached_value = value_layer\n",
    "        else:\n",
    "            last_hidden_state = hidden_states[:, -1:, :]\n",
    "            key_layer = self.key(last_hidden_state)\n",
    "            value_layer = self.value(last_hidden_state)\n",
    "            self.cached_key = torch.cat([self.cached_key, key_layer], dim=1)\n",
    "            self.cached_value = torch.cat([self.cached_value, value_layer], dim=1)\n",
    "        \n",
    "        return query_layer, self.cached_key, self.cached_value\n",
    "\n",
    "# Replace attention layers with cached quantized versions\n",
    "for layer in model_int8.transformer.h:\n",
    "    layer.attn = CachedQuantizedAttention(layer.attn)\n",
    "\n",
    "print(\"Applied quantized key-value caching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Use dynamic quantization for activations to handle varying sequence lengths\n",
    "\n",
    "Dynamic quantization is applied at runtime, so we'll implement a wrapper for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_quantized_inference(model, input_ids):\n",
    "    quantized_model = torch.quantization.quantize_dynamic(\n",
    "        model, {torch.nn.Linear}, dtype=torch.qint8\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        return quantized_model(input_ids)\n",
    "\n",
    "print(\"Implemented dynamic quantization for activations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Deploy using an inference-optimized runtime like ONNX Runtime\n",
    "\n",
    "For this step, we'll export our model to ONNX format and use ONNX Runtime for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_onnx(model, input_shape):\n",
    "    dummy_input = torch.randint(0, 1000, input_shape)\n",
    "    torch.onnx.export(model, dummy_input, \"quantized_gpt2.onnx\",\n",
    "                      input_names=['input_ids'],\n",
    "                      output_names=['logits'],\n",
    "                      dynamic_axes={'input_ids': {0: 'batch_size', 1: 'sequence'},\n",
    "                                    'logits': {0: 'batch_size', 1: 'sequence'}})\n",
    "    print(\"Model exported to ONNX format\")\n",
    "\n",
    "def onnx_inference(onnx_path, input_ids):\n",
    "    session = ort.InferenceSession(onnx_path)\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    output_name = session.get_outputs()[0].name\n",
    "    return session.run([output_name], {input_name: input_ids.numpy()})[0]\n",
    "\n",
    "# Export the model to ONNX\n",
    "export_to_onnx(model_int8, (1, 512))\n",
    "\n",
    "# Perform inference using ONNX Runtime\n",
    "input_ids = torch.randint(0, 1000, (1, 128))\n",
    "onnx_output = onnx_inference(\"quantized_gpt2.onnx\", input_ids)\n",
    "print(\"Performed inference using ONNX Runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Performance Benchmarking\n",
    "\n",
    "Finally, let's benchmark our optimized model against the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark(model, input_ids, runs=100):\n",
    "    start_time = time.time()\n",
    "    for _ in range(runs):\n",
    "        with torch.no_grad():\n",
    "            _ = model(input_ids)\n",
    "    end_time = time.time()\n",
    "    return (end_time - start_time) / runs\n",
    "\n",
    "input_ids = torch.randint(0, 1000, (1, 512)).to(device)\n",
    "\n",
    "original_time = benchmark(model, input_ids)\n",
    "quantized_time = benchmark(model_int8, input_ids)\n",
    "onnx_time = benchmark(lambda x: onnx_inference(\"quantized_gpt2.onnx\", x), input_ids.cpu())\n",
    "\n",
    "print(f\"Original model average inference time: {original_time:.4f} seconds\")\n",
    "print(f\"Quantized model average inference time: {quantized_time:.4f} seconds\")\n",
    "print(f\"ONNX Runtime average inference time: {onnx_time:.4f} seconds\")\n",
    "print(f\"Speedup: {original_time / onnx_time:.2f}x\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
